{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import deepcut\n",
    "from pathlib import Path\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense ,GRU ,TimeDistributed ,LSTM ,Bidirectional,Flatten ,Dropout\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt, mpld3\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/itoeiji/deep-reinforcement-learning-on-stock-data\n",
    "# https://github.com/rlcode/reinforcement-learning/blob/master/2-cartpole/1-dqn/cartpole_dqn.py\n",
    "\n",
    "จากหลาย Document การใช้ Dense จะใช้ relu เป็น activation และ layer  สุดท้ายให้ใช้ Dense โดยไม่ต้องใส่ Activation\n",
    "https://markelsanz14.medium.com/introduction-to-reinforcement-learning-part-3-q-learning-with-neural-networks-algorithm-dqn-1e22ee928ecd\n",
    "https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp2str(x):\n",
    "    return datetime.utcfromtimestamp(x).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/chakritphain/Downloads/TFEX_S50U2020_1_19-10_to 04-12.csv')\n",
    "dftmp = df[['open','high','low','close','Volume']]\n",
    "dftmp['Volume'] = dftmp.Volume.apply(lambda x: x.replace(\",\",\"\"))\n",
    "#dftmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmp.Volume  = dftmp.Volume.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value2Diff(series):\n",
    "    prev = series[:-1]\n",
    "    now = series[1:]\n",
    "    return np.concatenate(([0],now-prev))\n",
    "#def DiffClose()\n",
    "def ma(series,num):\n",
    "    tmp = series[num:].values\n",
    "    for i in range(1,num):\n",
    "        #print(i)\n",
    "        tmp = series[num-i:-i].values + tmp \n",
    "    tmp = tmp/num\n",
    "    avg = np.average(series)\n",
    "    tmp = np.concatenate((series[:num],tmp))\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmp['vma5'] = ma(dftmp.Volume,5)\n",
    "dftmp['vma15'] = ma(dftmp.Volume,15)\n",
    "dftmp['vma30'] = ma(dftmp.Volume,30)\n",
    "dftmp['cma5'] = ma(dftmp.close,5)\n",
    "dftmp['cma15'] = ma(dftmp.close,15)\n",
    "dftmp['cma30'] = ma(dftmp.close,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(ma(dftmp.Volume,5))\n",
    "#dftmp.Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = Value2Diff(dftmp.open.values)\n",
    "#o = dftmp.open - dftmp.close  #เปิด-ปิด\n",
    "#h = Value2Diff(dftmp.high.values)\n",
    "h = dftmp.high - dftmp.close #สูง-ปิด\n",
    "#l = Value2Diff(dftmp.low.values)\n",
    "l = dftmp.close - dftmp.low #ปิด-ต่ำ\n",
    "c = Value2Diff(dftmp.close.values) #ปิดตอนนี้-ปิดเมื่อกี้\n",
    "cd5 = dftmp.close - dftmp.cma5\n",
    "c5 = Value2Diff(dftmp.cma5.values)\n",
    "cd15 = dftmp.close - dftmp.cma15\n",
    "c15 = Value2Diff(dftmp.cma15.values)\n",
    "cd30 = dftmp.close - dftmp.cma30\n",
    "c30 = Value2Diff(dftmp.cma30.values)\n",
    "c5d15 = dftmp.cma5 - dftmp.cma15\n",
    "c5d30 = dftmp.cma5 - dftmp.cma30\n",
    "v = Value2Diff(dftmp.Volume.values) #ปิดตอนนี้-ปิดเมื่อกี้\n",
    "vd5 = dftmp.Volume - dftmp.vma5\n",
    "v5 = Value2Diff(dftmp.vma5.values)\n",
    "vd15  = dftmp.Volume - dftmp.vma15\n",
    "v15 = Value2Diff(dftmp.vma15.values)\n",
    "vd30  = dftmp.Volume - dftmp.vma30\n",
    "v30 = Value2Diff(dftmp.vma30.values)\n",
    "v5d15 = dftmp.vma5 - dftmp.vma15\n",
    "v5d30 = dftmp.vma5 - dftmp.vma30\n",
    "df_diff = pd.DataFrame({'o':o,'h':h,'l':l,'c':c,'v':v,'v5':v5,'v15':v15,'v30':v30,'vd5':vd5,'vd15':vd15,'vd30':vd30,'v5d15':v5d15,'v5d30':v5d30,'c5':c5,'c15':c15,'c30':c30,'cd5':cd5,'cd15':cd15,'cd30':cd30,'c5d15':c5d15,'c5d30':c5d30})\n",
    "display(df_diff.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_diff) #Make Model MinMaxScaler\n",
    "df_diff_norm = pd.DataFrame(scaler.transform(df_diff),columns = df_diff.columns)\n",
    "df_diff_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(np.array([[0,0,0,0.282132,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]))[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(df_diff_norm.iloc[0:20]).T[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(251).hist(df_diff['o'])\n",
    "plt.subplot(252).hist(df_diff['h'])\n",
    "plt.subplot(253).hist(df_diff['l'])\n",
    "plt.subplot(254).hist(df_diff['c'])\n",
    "plt.subplot(255).hist(df_diff['v'])\n",
    "plt.subplot(256).hist(df_diff_norm['o'])\n",
    "plt.subplot(257).hist(df_diff_norm['h'])\n",
    "plt.subplot(258).hist(df_diff_norm['l'])\n",
    "plt.subplot(259).hist(df_diff_norm['c'])\n",
    "plt.subplot(2,5,10).hist(df_diff_norm['v'])\n",
    "plt.subplot(251).legend(['o'])\n",
    "plt.subplot(252).legend(['h'])\n",
    "plt.subplot(253).legend(['l'])\n",
    "plt.subplot(254).legend(['c'])\n",
    "plt.subplot(255).legend(['v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LSTM\n",
    "import tensorflow as tf\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    #dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    #dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#for element in dataset:\n",
    "#    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "batch_size = 12\n",
    "shuffle_buffer_size = 10\n",
    "\n",
    "#data_series = np.array(list(windowed_dataset(df_diff_norm.iloc[:].values,window_size,batch_size,shuffle_buffer_size)))\n",
    "#data_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def westerncandlestick(ax, Data, width=0.2, colorup='k', colordown='r', linewidth=0.5):\n",
    "\n",
    "    Open, Hight, Low,Close, Volumn, v5 ,v15 ,v30,c5,c15,c30 = Data\n",
    "    Date = np.array(range(len(Open)))\n",
    "    Date  = Date.astype('float32')\n",
    "    Color = ['b']\n",
    "    for i in range(1,len(Date)): \n",
    "        Color.append(['r','k'][Open[i-1]<Close[i]])\n",
    "    OFFSET = .4\n",
    "\n",
    "    #Hight Low Lines\n",
    "    ax.vlines(Date,Low,Hight,Color)\n",
    "\n",
    "    #Open Lines\n",
    "    ax.hlines(Open,Date-OFFSET,Date,Color)\n",
    "    #Close Lines\n",
    "    ax.hlines(Close,Date,Date+OFFSET,Color)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_norm.iloc[:100].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromlim = -100\n",
    "lim = -1\n",
    "f,ax = plt.subplots(figsize=(20,10))\n",
    "ax.plot(list(range(len(dftmp.iloc[fromlim:lim]))),dftmp.iloc[fromlim:lim].cma5,'r-')\n",
    "ax.plot(list(range(len(dftmp.iloc[fromlim:lim]))),dftmp.iloc[fromlim:lim].cma15,'g-')\n",
    "ax.plot(list(range(len(dftmp.iloc[fromlim:lim]))),dftmp.iloc[fromlim:lim].cma30,'b-')\n",
    "#f.subplots_adjust(right=0.75)\n",
    "vol = ax.twinx()\n",
    "fromlim = -100\n",
    "lim = -1\n",
    "vol.bar(list(range(len(dftmp.iloc[fromlim:lim]))),dftmp.Volume.values[fromlim:lim])\n",
    "vol.plot(list(range(len(dftmp.iloc[fromlim:lim]))),dftmp.iloc[fromlim:lim].vma5,'r-')\n",
    "vol.plot(list(range(len(dftmp.iloc[fromlim:lim]))),dftmp.iloc[fromlim:lim].vma15,'g-')\n",
    "vol.plot(list(range(len(dftmp.iloc[fromlim:lim]))),dftmp.iloc[fromlim:lim].vma30,'b-')\n",
    "westerncandlestick(ax,dftmp.iloc[fromlim:lim].T.values)\n",
    "\n",
    "#host.set_xlim(0, 210)\n",
    "ax.set_ylim(dftmp.iloc[fromlim:lim].low.min()-10, dftmp.iloc[fromlim:lim].high.max())\n",
    "vol.set_ylim(0, dftmp.iloc[fromlim:lim].Volume.max()*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "#init_notebook_mode()\n",
    "#from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    def __init__(self, data,scaler):\n",
    "        self.data = data\n",
    "        #self.history_t = history_t\n",
    "        self.scaler = scaler\n",
    "        self.window_size = window_size+1\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        #self.profits = 0\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.buyed = 0 #0 = ยังไม่ซื้อ , 1 = L , 2 = S\n",
    "        #self.rewardSat = 0 มันคืออันเดียวกับ self.position_value เลือกอันใดอันหนึ่ง (มูลค่าสะสม)\n",
    "        #self.history = [0 for _ in range(self.history_t)]\n",
    "        #print(self.addstate(self.data[0],self.buyed,self.position_value))\n",
    "        #return [self.position_value] + self.history # obs\n",
    "        #print(self.data.iloc[0])\n",
    "        return self.addstate(self.data.iloc[0],self.buyed,self.position_value)\n",
    "    def plot(self,ax):\n",
    "        r = self.scaler.inverse_transform(self.data).T[3]\n",
    "        ax.plot(list(range(len(r))),r,'y+')\n",
    "        return\n",
    "    \n",
    "    def addstate(self,data_stage,buyed,position_value):\n",
    "        if buyed == 0:\n",
    "            state = np.append(data_stage.T,1) #\n",
    "            state = np.append(state.T,0) #\n",
    "            state = np.append(state.T,0) #\n",
    "        elif buyed == 1:\n",
    "            state = np.append(data_stage.T,0) #\n",
    "            state = np.append(state.T,1) #\n",
    "            state = np.append(state.T,0) #\n",
    "        elif buyed == 2:\n",
    "            state = np.append(data_stage.T,0) #\n",
    "            state = np.append(state.T,0) #\n",
    "            state = np.append(state.T,1) #            \n",
    "            \n",
    "        pv = self.ScalerPV(position_value)\n",
    "        \n",
    "        state = np.append(state,pv)\n",
    "        #print(state)\n",
    "        return state.T\n",
    "    #def buyedTransform(self,buyed):\n",
    "    #    return buyed/2\n",
    "    def ScalerPV(self,x):\n",
    "        return (x + 5) / 10\n",
    "    \n",
    "    def step(self, act):\n",
    "        # act = 0: stay, 1: buy, 2: sell\n",
    "        if act == 0:\n",
    "            reward = 0\n",
    "            if self.buyed == 1:\n",
    "                r = self.scaler.inverse_transform(np.array([[0,0,0,self.data.iloc[self.t]['c'],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]))[0,3]\n",
    "                self.positions.append(r)\n",
    "                #self.position_value = self.position_value + r\n",
    "            elif  self.buyed == 2:\n",
    "                r = self.scaler.inverse_transform(np.array([[0,0,0,self.data.iloc[self.t]['c'],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]))[0,3]\n",
    "                self.positions.append(r) \n",
    "                #self.position_value = self.position_value - r\n",
    "        elif act == 1: #buy L\n",
    "            if self.buyed == 1 or self.buyed == 2:\n",
    "                reward = -1 #ถ้ามีการซื้อแล้ว และมีการซื้อซ้ำ ให้ลบ 1 (ต้องขายก่อนจึงจะซืื้อใหม่)\n",
    "                r = self.scaler.inverse_transform(np.array([[0,0,0,self.data.iloc[self.t]['c'],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]))[0,3]\n",
    "                self.positions.append(r) #แต่ก็ยัง append state ปัจจุบันอยู่ดี\n",
    "            else:\n",
    "                #r = self.scaler.inverse_transform(np.array([[0,0,0,self.data.iloc[self.t]['c'],0]]))[0,3]\n",
    "                self.positions.append(0)\n",
    "                self.buyed = 1\n",
    "                reward = 0.01 #prefer buy more than stay #-0.3\n",
    "        elif act == 3: #buy S\n",
    "            if self.buyed == 1 or self.buyed == 2:\n",
    "                reward = -1 #ถ้ามีการซื้อแล้ว และมีการซื้อซ้ำ ให้ลบ 1 (ต้องขายก่อนจึงจะซืื้อใหม่)\n",
    "                r = self.scaler.inverse_transform(np.array([[0,0,0,self.data.iloc[self.t]['c'],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]))[0,3]\n",
    "                self.positions.append(r) #แต่ก็ยัง append state ปัจจุบันอยู่ดี\n",
    "            else:\n",
    "                #r = self.scaler.inverse_transform(np.array([[0,0,0,self.data.iloc[self.t]['c'],0]]))[0,3]\n",
    "                self.positions.append(0)\n",
    "                self.buyed = 2\n",
    "                reward = 0.01 #prefer buy more than stay #-0.3        \n",
    "        elif act == 2: # sell ตอนขายจึงจะได้ Reward\n",
    "            if self.buyed == 0:\n",
    "                reward = -1 #ถ้ายังไม่เคยซื้อแล้วกดขายให้ลบ 1 (ต้องซืื้อก่อนจึงจะขายได้)\n",
    "            elif self.buyed == 1:#L\n",
    "                profits = 0\n",
    "                for p in self.positions:\n",
    "                    profits += p\n",
    "                reward = profits - 0.3\n",
    "                self.positions = []\n",
    "                self.position_value = 0\n",
    "                self.buyed = 0                \n",
    "            elif self.buyed == 2:#S\n",
    "                profits = 0\n",
    "                for p in self.positions:\n",
    "                    profits += p\n",
    "                reward = -profits - 0.3\n",
    "                self.positions = []\n",
    "                self.position_value = 0\n",
    "                self.buyed = 0   \n",
    "        \n",
    "        # set next time\n",
    "        #print(self.t,len(self.data))\n",
    "        if self.t >= len(self.data)-1:\n",
    "            self.done = True\n",
    "            state = self.addstate(self.data.iloc[self.t, :],self.buyed,self.position_value)\n",
    "            return state ,reward, self.done\n",
    "\n",
    "        self.t += 1\n",
    "        self.position_value = 0\n",
    "        for p in self.positions:\n",
    "            if p > 0:\n",
    "                stay_rate  = 0.001\n",
    "            else:\n",
    "                stay_rate  = -0.001\n",
    "            self.position_value = self.position_value + p + stay_rate\n",
    "            \n",
    "        if self.buyed == 1:\n",
    "            self.position_value = self.position_value\n",
    "        elif self.buyed == 2:\n",
    "            self.position_value = -self.position_value    \n",
    "            \n",
    "        if self.position_value < -5: #บังคับขาย ถ้าขาดทุนเกิน 1  บาทให้ขายทันที (ไม่อยากให้ self.position_value ต่ำกว่า 1 บาทด้วย)\n",
    "            #เพราะใช้ Relu ค่าเลยติดลบไม่ได้\n",
    "            reward += -6\n",
    "        \n",
    "        #จำนวน Array ของ self.positions คือค่า Stay ยิ่งนานก็ให้ rewardเยอะ\n",
    "        #เนื่องจาก Deep Learning ต้องการแค่เลข 0-1 จึงสามารถกำหนดได้เพียง + และ - โดย หากติดลบให้ 0 หากเป็น + ให้ 1\n",
    "        #self.position_value = 0 if self.position_value < 0 else 1\n",
    "        #self.history.pop(0)\n",
    "        #self.history.append(self.data.iloc[self.t]['c'] - self.data.iloc[(self.t-1)]['c'])\n",
    "        #fc = self.data.iloc[self.t, :].c\n",
    "        #fo = self.data.iloc[self.t, :].o\n",
    "        #fl = self.data.iloc[self.t, :].l\n",
    "        #fh = self.data.iloc[self.t, :].h\n",
    "        #fv = self.data.iloc[self.t, :].v \n",
    "        #return [self.position_value] + self.history, reward, self.done # obs, reward, done\n",
    "        #จริงๆ ควรเอา self.position_value เป็น state ด้วย\n",
    "        #แต่ติดที่ค่ามันมากกว่า 1 เข้า deep learning แล้วอาจมีปัญหาเพราะยังไม่ได้ Normalize\n",
    "        #จึงเอาเท่านี้ก่อน\n",
    "        #return [fo,fh,fl,fc,fv,self.buyed,self.position_value], reward, self.done\n",
    "        return self.addstate(self.data.iloc[self.t, :],self.buyed,self.position_value) ,reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment1(df_diff_norm,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment1(df_diff_norm,scaler)\n",
    "act = ['stay','buy','sell']\n",
    "print(env.reset())\n",
    "for _ in range(3):\n",
    "    pact = np.random.randint(3)\n",
    "    print(act[pact])\n",
    "    print(env.step(pact))\n",
    "env.plot(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 25\n",
    "action_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "#del(model)\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim=state_size, activation='sigmoid', kernel_initializer='he_uniform'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(4, input_dim=state_size, activation='sigmoid', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(action_size, activation='linear',kernel_initializer='he_uniform'))\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer=Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  datetime\n",
    "def append_sample(state, action, reward, next_state, done,epsilon_min,epsilon_decay,epsilon,memory):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    return [epsilon,memory]\n",
    "\n",
    "def train_model(model,memory,train_start,batch_size,state_size,discount_factor):\n",
    "    if len(memory) < train_start:\n",
    "        return\n",
    "    batch_size = min(batch_size, len(memory)) #ค่าไหนน้อยกว่ากันเอาค่านั้นระหว่าง batch_size  และ จำนวน data ใน memory\n",
    "    mini_batch = random.sample(memory, batch_size) # สุ่มจาก memory มาเท่ากับจำนวน batch_size เพื่อเอาไป Train\n",
    "\n",
    "    state = np.zeros((batch_size,state_size))\n",
    "    next_state= np.zeros((batch_size, state_size))\n",
    "    action, reward, done = [], [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        state[i] = mini_batch[i][0]\n",
    "        action.append(mini_batch[i][1])\n",
    "        reward.append(mini_batch[i][2])\n",
    "        next_state[i] = mini_batch[i][3]\n",
    "        done.append(mini_batch[i][4])\n",
    "\n",
    "    state_Q = model.predict(state) #สร้างตาราง Q-Table ขึ้นมา\n",
    "\n",
    "    next_state_Q = model.predict(next_state)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Q Learning: get maximum Q value at s' from target model\n",
    "        if done[i]:\n",
    "            state_Q[i][action[i]] = reward[i] #ใส่ Reward ที่ Q-Table\n",
    "        else:\n",
    "            #กุญแจของ Q Learning อยู่ตรงนี้\n",
    "            state_Q[i][action[i]] = reward[i] + discount_factor * (np.amax(next_state_Q[i]))\n",
    "    log_dir = \"../data/logs/stock_ql_V4.0_25dim1Hidden/\" + datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # and do the model fit!\n",
    "    filename = \"./stock_ql_V4.0_25dim.f5\"\n",
    "    if os.path.isfile(filename+\".index\"):\n",
    "        #print(\"Load ../dql_stock_ql_V3_weight.f5\")\n",
    "        a = model.load_weights(filename)\n",
    "        #print(a)\n",
    "    history = model.fit(state, state_Q, batch_size=batch_size,epochs=30, verbose=0,callbacks=[tensorboard_callback])\n",
    "    model.save_weights(filename)\n",
    "    return [model,history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fromlim = random.randint(-1000,-101)\n",
    "lim = fromlim+100\n",
    "env = Environment1(df_diff_norm.iloc[fromlim:lim],scaler)\n",
    "# Hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 0.8\n",
    "epsilon = 0.5\n",
    "epsilon_decay = 0.99999\n",
    "discount_factor = 0.9\n",
    "epsilon_min = 0.01 #0.01\n",
    "train_start = 10\n",
    "batch_size  = 100\n",
    "total_action = 12\n",
    "results  = []\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "memory = []\n",
    "scores = []\n",
    "episodes = []\n",
    "mse_loss = []\n",
    "fromlim = -822\n",
    "lim = -722\n",
    "for i in range(1,10000+1):\n",
    "    fromlim = random.randint(-1000,-101)\n",
    "    lim =  fromlim+100\n",
    "    print('Random=',fromlim,lim,end=\" \")\n",
    "    state = np.zeros(state_size)\n",
    "    score = 0\n",
    "    arr_score = []\n",
    "    arr_reward = []\n",
    "    arr_position_v = []\n",
    "    arr_state = []\n",
    "    arr_action = []\n",
    "    epochs, penalties, reward, s = 0, 0, 0 ,0\n",
    "    #epsilon = 0.1\n",
    "    done = False\n",
    "    del(env)\n",
    "    env = Environment1(df_diff_norm.iloc[fromlim:lim],scaler)\n",
    "    env.reset()\n",
    "    if i % 100  ==  0:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        ax1 = plt.subplot(411)\n",
    "        ax2 = plt.subplot(412)\n",
    "        ax3 = plt.subplot(413)\n",
    "        ax4 = plt.subplot(414)\n",
    "        vol = ax1.twinx()\n",
    "        ax1.plot(list(range(lim-fromlim)),dftmp.iloc[fromlim:lim]['close'])\n",
    "        ax1.plot(list(range(lim-fromlim)),dftmp.iloc[fromlim:lim].cma5,'r-')\n",
    "        ax1.plot(list(range(lim-fromlim)),dftmp.iloc[fromlim:lim].cma15,'g-')\n",
    "        vol.bar(list(range(lim-fromlim)),dftmp.iloc[fromlim:lim].Volume)\n",
    "        vol.plot(list(range(lim-fromlim)),dftmp.iloc[fromlim:lim].vma5,'r-')\n",
    "        vol.plot(list(range(lim-fromlim)),dftmp.iloc[fromlim:lim].vma15,'g-')\n",
    "        ax1.set_ylim(dftmp.iloc[fromlim:lim].low.min()-10, dftmp.iloc[fromlim:lim].high.max())\n",
    "        vol.set_ylim(0, dftmp.iloc[fromlim:lim].Volume.max()*2)\n",
    "    while not done:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = random.randrange(action_size)\n",
    "            arr_action_tmp = np.zeros(action_size)\n",
    "            color = ['dont care','g+','r+','b+']\n",
    "        else:\n",
    "            color = ['dont care','go','ro','bo']\n",
    "            #action = np.argmax(Qtable[state]) # Exploit learned values\n",
    "            arr_action_tmp =  model.predict(np.array(state).reshape(1,-1))[0]\n",
    "            action = np.argmax(arr_action_tmp)\n",
    "        arr_action.append(arr_action_tmp)\n",
    "        if action != 0:\n",
    "            ax1.plot(s,dftmp.iloc[fromlim+s]['close'],color[action]) \n",
    "        s += 1 \n",
    "        next_state, reward, done = env.step(action)\n",
    "        #print(next_state)\n",
    "        score += reward\n",
    "        arr_reward.append(reward)\n",
    "        arr_score.append(score)\n",
    "        arr_position_v.append(next_state[-1])\n",
    "        #state2 = np.reshape(state, [1, state_size])\n",
    "        #next_state2 = np.reshape(next_state, [1, state_size]) #DQN\n",
    "        epsilon,memory = append_sample(state, action, reward, next_state, done,epsilon_min,epsilon_decay,epsilon,memory) #DQN\n",
    "        state = next_state\n",
    "        arr_state.append(state)\n",
    "        #old_value = Qtable[state,action]\n",
    "        #next_max = np.max(Qtable[next_state])\n",
    "        #new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        #Qtable[state,action] = new_value\n",
    "        #print(model,memory,train_start,batch_size,state_size,discount_factor)\n",
    "    if i % 100  ==  0:\n",
    "        ax2.plot(arr_reward,'b+')\n",
    "        ax2.plot(arr_score,'m-')\n",
    "        ax2.plot(arr_position_v,'r.')\n",
    "        ax2.axhline(y=0.0, color='g', linestyle='-')\n",
    "        ax2.axhline(y=-5, color='b', linestyle='-')\n",
    "        env.plot(ax2)\n",
    "        ax2.legend([\"reward (P&L)\",\"arr_reward_cum (P&L)\"])\n",
    "        ax3.plot(arr_action)\n",
    "        ax3.legend(['Stay','L','Sell','S'])\n",
    "        ax4.plot(arr_state)\n",
    "        ax4.legend(['o','h','l','c','v','v5','v15','v30','v5d15','v5d30','c5','c15','c30','c5d15','c5d30','stay','L','S','pValue'])\n",
    "\n",
    "    print(\"Epoch=\" +str(i) + \", Epsilon=%.2f\"%epsilon + \",Score=%.2f\"%score)\n",
    "    plt.show(arr_state)\n",
    "    scores.append(score)\n",
    "    #print(\"Score=%.2f,%.2f\" % (score , epsilon))\n",
    "    result = train_model(model,memory,train_start,batch_size,state_size,discount_factor)\n",
    "    if result:\n",
    "        model,history = result\n",
    "        mse_loss.append(history.history['loss'])\n",
    "        #print('loss = ',history.history['loss'][0])\n",
    "    if i % 10  ==  0:\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.plot(mse_loss[:])\n",
    "        plt.axhline(y=0.05, color='b', linestyle='-')\n",
    "        plt.legend([\"mse_loss\"])\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.plot(scores[:])\n",
    "        plt.axhline(y=0.0, color='b', linestyle='-')\n",
    "        plt.legend([\"Scores\"])\n",
    "        #plt.ylim(0,10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
